---
title: "Selekcja cech dla modeli liniowych"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(MASS)
library(ISLR)
library(leaps)
```

## Selekcja cech dla modeli liniowych

Używamy zbioru danych `Hitters` z pakietu `ISLR` (`?Hitters`). Należy
usunąć wiersze zawierające wartości `NA`.

```{r na.omit}
Hitters <- na.omit(Hitters)
```

Metody selekcji cech są zaimplementowane w funkcji `regsubsets()` z
pakietu `leaps`.

### Wybór najepszego podzbioru

Metoda regsubsets zwraca modele o najlepszym dopasowaniu formuły dla
różnych liczb predyktorów.

```{r bestSubsets1}
Hitters_sub <- regsubsets(Salary ~ ., data = Hitters)
summary(Hitters_sub)
```

Jak można zobaczyć, funkcja `regsubsets()` domyślnie uwzględnia
maksymalnie 8 predyktorów. Jeśli chcemy to zmienić, musimy użyć
parametru `nvmax`.

```{r bestSubsets}
Hitters_sub <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
Hitters_sub_sum <- summary(Hitters_sub)
Hitters_sub_sum
```

Cechy selekcjonowane są niezależnie dla każdej liczby parametrów, ale
obiekt zwracany przez funkcję `summary.regsubsets()` zawiera informacje
umożliwiające zidentyfikowanie globalnie najlepszego pozdbioru cech, np.
miarę $C_p$.

```{r cp}
Hitters_sub_sum$cp
```

Najlepszy podzbiór według kryterium BIC

```{r bestBIC}
bic_min <- which.min(Hitters_sub_sum$bic)
bic_min
Hitters_sub_sum$bic[bic_min]
```

Stosowny obrazek

```{r bestBICPlot}
plot(Hitters_sub_sum$bic, xlab = "Liczba zmiennych", ylab = "BIC", col = "green",
     type = "b", pch = 20)
points(bic_min, Hitters_sub_sum$bic[bic_min], col = "red", pch = 9)
```

Dostępny jest też specjalny rodzaj wykresu (`?plot.regsubsets`).

```{r regsubsetsPlot}
plot(Hitters_sub, scale = "bic")
```

Estymaty współczynników dla optymalnego podzbioru

```{r bestSubsetCoef}
coef(Hitters_sub, id = 6)
```

W pythonie w bibliotekach, z których korzystamy nie ma bezpośredniego
odopwiednika funkcji regsubsets (można doinstalować bibliotekę mlxtend
<https://github.com/rasbt/mlxtend>, gdzie jest dostępny exhaustive
search). Ma to uzasadnienie praktyczne, gdyż taki rodzaj selekcji jest
bardzo wymagający obliczeniowo. Zaimplementujmy analogiczną
funkcjonalność wykorzystując bibliotekę statsmodels:

```{python stepwise}
import statsmodels.formula.api as smf
import itertools

hitters_df = r.Hitters
X = hitters_df.columns.drop('Salary')
nvmax = 3

results = []
for k in range(1, min(nvmax, len(X)) + 1):
    for combo in itertools.combinations(X, k): # Musimy ręcznie utworzyć kombinacje
        formula = 'Salary ~ ' + ' + '.join(combo) # Tworzenie formuł
        model = smf.ols(formula, data=hitters_df).fit()
        results.append({
            'n_vars': k,
            'vars': combo,
            'model': model,
        })

# Najlepsze modele dla każdej kombinacji selekcjonujemy również ręcznie
best_models = {}
for n in range(1, min(nvmax, len(X)) + 1):
    subset = [r for r in results if r['n_vars'] == n]
    best = max(subset, key=lambda x: x['model'].rsquared)
    best_models[n] = best
```

```{python result}
for nvar, model in best_models.items():
  print(f"{nvar}: variables: {model['vars']}, bic: {model['model'].bic}, aic: {model['model'].aic}")
```

```{r}
cp_min <- which.min(Hitters_sub_sum$cp)
cp_min
Hitters_sub_sum$cp[cp_min]
```

```{r}
plot(Hitters_sub_sum$cp, xlab = "Liczba zmiennych", ylab = "CP", col = "green",
     type = "b", pch = 20)
points(cp_min, Hitters_sub_sum$cp[cp_min], col = "red", pch = 9)
```

```{r}
adjr2_max <- which.max(Hitters_sub_sum$adjr2)
adjr2_max
Hitters_sub_sum$adjr2[adjr2_max]
```

```{r}
plot(Hitters_sub_sum$adjr2, xlab = "Liczba zmiennych", ylab = "Adjusted R^2", col = "green",
     type = "b", pch = 20)
points(adjr2_max, Hitters_sub_sum$adjr2[adjr2_max], col = "red", pch = 9)
```

[**Wykonaj podobną analizę dla innych kryteriów optymalności:** $C_p$ i
poprawionego $R^2$. Zwróć uwagę na to, że poprawione $R^2$ powinno być
*zmaksymalizowane*.]

### Selekcja krokowa do przodu i wstecz

Funkcja `regsubsets()` z odpowiednio ustawionym parametrem `method` może
przeprowadzić selekcję krokową.

```{r stepwise}
Hitters_fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, 
                          method = "forward")
Hitters_fwd_sum <- summary(Hitters_fwd)
Hitters_fwd_sum
Hitters_back <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, 
                           method = "backward")
Hitters_back_sum <- summary(Hitters_back)
Hitters_back_sum
```

W tym wypadku odpowienik metody mamy już dostępny w scikit-learn razem z
zestawem innych selektorów
(<https://scikit-learn.org/stable/modules/feature_selection.html>):

```{python stepwise}
import numpy as np
import pandas as pd
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

X = hitters_df.drop('Salary', axis=1)
y = hitters_df['Salary']

# Handle categorical variables if needed
X = pd.get_dummies(X, drop_first=True)

# W scikit-learn nie mamy formuły, więc deklarujemy odpowiedni model
model = LinearRegression()

sfs = SequentialFeatureSelector(
    model, 
    n_features_to_select=5,
    direction='forward',  # Selekcja do przodu
    scoring='neg_mean_squared_error',  # Metoda porównawcza dla równej liczności
    cv=5  # Wychodząc trochę do przodu, możemy dodać walidację krzyżową
)

sfs.fit(X, y)

# Otrzymujemy maskę wybranych cech
sfs.get_support()
```

SFS jest transformerem, tzn. operacją, która przetransformuje dane
wejściowe. Ma to zastosowanie w implementowaniu modeli w formie
pipeline'ów. Innymi rodzajami transformerów są na przykład skalowanie
cech czy kodowanie danych kategorycznych. W przypadku tabel pandas
natomiast wygodnie będzie nałożyć maskę binarną na kolumny tabeli.

```{python transform}
X_sel = X.loc[:, sfs.get_support()]
X_sel
```

[**Które podzbiory predyktorów są optymalne w selekcji krokowej w przód
i wstecz według kryteriów BIC,** $C_p$ i poprawionego $R^2$? Czy któreś
z nich są faktycznie najlepsze?]

```{r cp}
Hitters_fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
Hitters_fwd_sum <- summary(Hitters_fwd)
best <- which.min(Hitters_fwd_sum$cp)
print(sum(xor(Hitters_fwd_sum$which[best, ], Hitters_sub_sum$which[best, ])))

Hitters_bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
Hitters_bwd_sum <- summary(Hitters_fwd)
best <- which.min(Hitters_fwd_sum$cp)
print(sum(xor(Hitters_fwd_sum$which[best, ], Hitters_sub_sum$which[best, ])))
```

```{r}

```

### Wybór modelu przy pomocy metody zbioru walidacyjnego

Estymaty błędów testowych będą dokładne tylko jeśli wszystkie aspekty
dopasowania modelu --- w tym selekcję zmiennych --- przeprowadzimy z
użyciem wyłącznie **zbioru uczącego**.

```{r valSet}
n <- nrow(Hitters)
train <- sample(c(TRUE, FALSE), n, replace = TRUE)
test <- !train
Hitters_bs_v <- regsubsets(Salary ~ ., data = Hitters[train,], nvmax = 19)
```

Niestety dla modeli zwracanych przez `regsubsets` nie ma odpowiedniej
metody `predict()`. Może ona mieć następującą postać (funkcja
`model.matrix()` tworzy macierz $X$ dla podanych punktów).

```{r predict.regsubsets}
predict.regsubsets <- function(object, newdata, id, ...) {
  model_formula <- as.formula(object$call[[2]])
  mat <- model.matrix(model_formula, newdata)
  coefs <- coef(object, id = id)
  mat[, names(coefs)] %*% coefs
}
```

Liczymy estymaty błędów

```{r valSetErrors}
prediction_error <- function(i, model, subset) {
  pred <- predict(model, Hitters[subset,], id = i)
  mean((Hitters$Salary[subset] - pred)^2)
}
val_errors <- sapply(1:19, prediction_error, model = Hitters_bs_v, subset = test)
val_errors
```

[**Ile zmiennych zawiera model optymalny?**]

Po ustaleniu optymalnej liczby zmiennych szukamy optymalnego modelu z tą
liczbą zmiennych **przy pomocy wszystkich obserwacji**.

### Wybór modelu przy pomocy $k$-krotnej walidacji krzyżowej

Musimy dopasować model na każdym z $k$ zbiorów uczących i policzyć błędy
testowe na odpowiednich zbiorach testowych.

```{r kcv}
k <- 10
folds <- sample(1:k, n, replace = TRUE)
val_err <- NULL
for (j in 1:k) {
  fit_bs <- regsubsets(Salary ~ ., data = Hitters[folds != j,], nvmax = 19)
  err <- sapply(1:19, prediction_error, model = fit_bs, subset = (folds == j))
  val_err <- rbind(val_err, err)
}
```

Estymata błędu CV jest teraz średnią błędów w każdej grupie.

```{r kcvErrors}
cv_errors <- colMeans(val_err)
cv_errors
```

[**Ile zmiennych ma model optymalny według tego kryterium?**]

Podobnie jak poprzednio, po wyznaczeniu optymalnej liczby zmiennych
szukamy optymalnego modelu z tą liczbą zmiennych przy pomocy całego
zbioru obserwacji.

## Regularyzacja

Obie omawiane na wykładzie metody regularyzacji są zaimplementowane w
funkcji `glmnet()` z pakietu `glmnet` oraz w module linear_model
biblioteki scikit-learn.

```{r glmnet}
library(glmnet)
```

Poza poznanymi już uogólnionymi modelami liniowymi pakiet ten dostarcza
funkcjonalności regresji grzbietowej i lasso. Ćwiczenia wykorzystują
zbiór danych `Hitters` z pakietu `ISLR`. **1. Przed wykonaniem ćwiczeń
należy z niego usunąć wiersze zawierające `NA`**.

```{r remove Na}
```

Funkcja `glmnet::glmnet()` ma inny interfejs od `lm()` i jej podobnych.
Trzeba w szczególności samodzielnie skonstruować macierz $\mathbf{X}$

```{r modelmatrix}
X <- model.matrix(Salary ~ ., data = Hitters)[, -1]
y <- Hitters$Salary
```

Argument `alpha` funkcji `glmnet()` decyduje o typie użytej
regularyzacji: `0` oznacza regresję grzbietową, a `1` lasso.

### Regresja grzbietowa

Wykonujemy regresję grzbietową dla jawnie określonych wartości
$\lambda$. *Podany ciąg* $\lambda$ powinien być malejący. Funkcja
`glmnet()` domyślnie dokonuje standaryzacji zmiennych.

```{r ridge}
lambda_grid <- 10^seq(10, -2, length.out = 100)
fit_ridge <- glmnet(X, y, alpha = 0.0, lambda = lambda_grid)
```

Dla każdej wartości $\lambda$ otrzymujemy zestaw estymat predyktorów
dostępnych w postaci macierzy

```{r ridgecoefs}
dim(coef(fit_ridge))
```

Można sprawdzić, że większe wartości $\lambda$ dają mniejszą normę
euklidesową współczynników (pomijamy wyraz wolny).

```{r ridgeCoefNormSmall}
fit_ridge$lambda[50]
coef_ridge <- coef(fit_ridge)[, 50]
coef_ridge
sqrt(sum(coef_ridge[-1]^2))
```

Natomiast mniejsze wartości $\lambda$ dają większą normę euklidesową
współczynników

```{r ridgeCoefNormBig}
fit_ridge$lambda[70]
coef(fit_ridge)[, 70]
sqrt(sum(coef(fit_ridge)[-1, 70]^2))
```

Przy pomocy funkcji `predict.glmnet()` można uzyskać np. wartości
estymat współczynników dla nowej wartości $\lambda$ (np. 50)

```{r predictGlmnet}
predict(fit_ridge, s = 50, type = "coefficients")
```

Estymujemy testowy MSE

```{r ridgemse}
set.seed(1)
n <- nrow(X)
train <- sample(n, n / 2)
test <- -train
fit_ridge <- glmnet(X[train,], y[train], alpha = 0, lambda = lambda_grid,
                    thresh = 1e-12)
```

Dla $\lambda = 4$

```{r ridgemse4}
pred_ridge <- predict(fit_ridge, s = 4, newx = X[test,])
mean((pred_ridge - y[test])^2)
```

Testowy MSE dla modelu zerowego (sam wyraz wolny)

```{r ridgenullmse}
pred_null <- mean(y[train])
mean((pred_null - y[test])^2)
```

Testowy MSE dla bardzo dużej wartości $\lambda = 10^{10}$

```{r ridgemse1e10}
pred_ridge_big <- predict(fit_ridge, s = 1e10, newx = X[test,])
mean((pred_ridge_big - y[test])^2)
```

[**Jakie wnioski możemy wyciągnąć z tego porównania?**]

Testowy MSE dla $\lambda = 0$ (metoda najmniejszych kwadratów)

```{r ridgemse0}
pred_ridge_0 <- predict(fit_ridge, x = X[train,], y = y[train], s = 0, 
                      newx = X[test,], exact = TRUE)
mean((pred_ridge_0 - y[test])^2)
```

Porównanie estymat współczynników

```{r ridgelm}
lm(y ~ X, subset = train)
predict(fit_ridge, x = X[train,], y = y[train], s = 0, exact = TRUE, 
        type = "coefficients")[1:20,]
```

Wyliczenie optymalnej wartości $\lambda$ przy pomocy walidacji krzyżowej

```{r ridgecv}
set.seed(1)
cv_out <- cv.glmnet(X[train,], y[train], alpha = 0)
plot(cv_out)
cv_out$lambda.min
```

MSE dla optymalnego $\lambda$

```{r ridgemsemin}
pred_ridge_opt <- predict(fit_ridge, s = cv_out$lambda.min, newx = X[test,])
mean((pred_ridge_opt - y[test])^2)
```

Estymaty współczynników dla optymalnego $\lambda$

```{r ridgecoefsmin}
fit_ridge_full <- glmnet(X, y, alpha = 0)
predict(fit_ridge_full, s = cv_out$lambda.min, type = "coefficients")
```

### Lasso

Dopasowujemy lasso dla ustalonej siatki parametrów regularyzacji

```{r lasso}
fit_lasso <- glmnet(X[train,], y[train], alpha = 1)
plot(fit_lasso, xvar = "lambda")
```

Wykonujemy walidację krzyżową i liczymy estymatę MSE

```{r lasso.cv.mse}
cv_out <- cv.glmnet(X[train,], y[train], alpha = 1)
plot(cv_out)
cv_out$lambda.min
pred_lasso <- predict(fit_lasso, s = cv_out$lambda.min, newx = X[test,])
mean((pred_lasso - y[test])^2)
```

[**Jak wygląda porównanie z modelem zerowym, metodą najmniejszych
kwadratów i regresją grzbietową?**]

Estymaty współczynników dla optymalnego $\lambda$

```{r lasso.coefs.min}
fit_lasso_full <- glmnet(X, y, alpha = 1)
predict(fit_lasso_full, s = cv_out$lambda.min, type = "coefficients")[1:20,]
```

I jeszcze wersja pythonowa w scikit-learn. Najpierw zaimportowanie
bibliotek.

```{python}
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from sklearn.linear_model import Lasso, Ridge, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
```

Następnie przygotowanie zbioru danych

```{python}
np.random.seed(42)

X = hitters_df.drop('Salary', axis=1)
y = hitters_df['Salary']

# Handle categorical variables if needed
X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

Trening

```{python}
alphas = [0.001, 0.01, 0.1, 1, 10]

ridge_results = {'alpha': [], 'coef': [], 'mse': [], 'r2': []}
lasso_results = {'alpha': [], 'coef': [], 'mse': [], 'r2': []}

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    y_pred = ridge.predict(X_test_scaled)
    
    ridge_results['alpha'].append(alpha)
    ridge_results['coef'].append(ridge.coef_)
    ridge_results['mse'].append(mean_squared_error(y_test, y_pred))
    ridge_results['r2'].append(r2_score(y_test, y_pred))

for alpha in alphas:
    lasso = Lasso(alpha=alpha)
    lasso.fit(X_train_scaled, y_train)
    y_pred = lasso.predict(X_test_scaled)
    
    lasso_results['alpha'].append(alpha)
    lasso_results['coef'].append(lasso.coef_)
    lasso_results['mse'].append(mean_squared_error(y_test, y_pred))
    lasso_results['r2'].append(r2_score(y_test, y_pred))

lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
lr_y_pred = lr.predict(X_test_scaled)
lr_mse = mean_squared_error(y_test, lr_y_pred)
```

I wizualizacja wag predyktorów

```{python}
feature_names = ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI']

ridge_coef_data = []
for i, alpha in enumerate(alphas):
    for j, feature in enumerate(feature_names):
        ridge_coef_data.append({
            'Alpha': alpha,
            'Coefficient': ridge_results['coef'][i][j],
            'Feature': feature,
            'Model': 'Ridge'
        })

lasso_coef_data = []
for i, alpha in enumerate(alphas):
    for j, feature in enumerate(feature_names):
        lasso_coef_data.append({
            'Alpha': alpha,
            'Coefficient': lasso_results['coef'][i][j],
            'Feature': feature,
            'Model': 'Lasso'
        })

coef_data = pd.concat([pd.DataFrame(ridge_coef_data), pd.DataFrame(lasso_coef_data)])

fig2 = make_subplots(rows=1, cols=2, subplot_titles=("Ridge Coefficients", "Lasso Coefficients"))

for feature in feature_names:
    ridge_data = coef_data[(coef_data['Model'] == 'Ridge') & (coef_data['Feature'] == feature)]
    fig2.add_trace(
        go.Scatter(x=ridge_data['Alpha'], y=ridge_data['Coefficient'], 
                   mode='lines+markers', name=feature),
        row=1, col=1
    )
    
    lasso_data = coef_data[(coef_data['Model'] == 'Lasso') & (coef_data['Feature'] == feature)]
    fig2.add_trace(
        go.Scatter(x=lasso_data['Alpha'], y=lasso_data['Coefficient'], 
                   mode='lines+markers', name=feature, showlegend=False),
        row=1, col=2
    )

fig2.update_layout(
    title='Coefficient Values vs Regularization Strength',
    xaxis_type='log',
    xaxis2_type='log'
)

fig2.update_xaxes(title_text='Alpha', row=1, col=1)
fig2.update_xaxes(title_text='Alpha', row=1, col=2)
fig2.update_yaxes(title_text='Coefficient Value', row=1, col=1)
fig2.update_yaxes(title_text='Coefficient Value', row=1, col=2)

fig2.show()
```

[**Jak teraz wygląda porównanie z regresją grzbietową?**]

### Elastic-Net

Wspomniany wcześniej argument `alpha` funkcji `glmnet()` wcale nie jest
parametrem binarnym a współczynnikiem wagi przypisanej do składnika
penalizującego regresji lasso podczas, gdy `1 - alpha` przypisane jest
do regresji grzbietowej.

Ważona suma obu tych składników penalizujących nazywa się Elastic-Net i
jest rodzajem kompromisu między własnościami regresji grzbietowej i
lasso.

[**Dopasuj do siatki parametrów model regularyzacji Elastic-Net o
wybranej wartości wagi `alpha` i przeprowadź podobna analizę, co dla
regularyzacji Lasso.**]
