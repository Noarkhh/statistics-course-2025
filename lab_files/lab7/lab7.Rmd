---
title: "Drzewa decyzyjne i modele pochodne"
output: html_document 
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(ISLR)
library(MASS)
library(tree)
library(randomForest)
```

```{python setup}
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.datasets import get_rdataset
import numpy as np
import pandas as pd
```

## Drzewa decyzyjne

Drzewa decyzyjne są zaimplementowane w pakiecie `tree` (nieco odmienna
implementacja dostępna jest w pakiecie `rpart`).

### Drzewa klasyfikacyjne

Poniższy kod wykorzystuje zbiór danych `Carseats` z pakietu `ISLR`.
Będziemy klasyfikować obserwacje do dwóch klas: *wysoka sprzedaż* i
*niska sprzedaż*. Uzupełniamy zbiór danych

```{r CarseatsDS}
High <- factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
CarseatsH <- data.frame(Carseats, High)
```

To samo inaczej

```{r within}
CarseatsH <- within(Carseats, High <- factor(ifelse(Sales <= 8, "No", "Yes")))
```

i jeszcze inaczej

```{r transform}
CarseatsH <- transform(Carseats, High = factor(ifelse(Sales <= 8, "No", "Yes")))
```

Budujemy drzewo klasyfikacyjne do predykcji `High` na podstawie
pozostałych zmiennych (poza `Sales`).

```{r classTree}
sales_high_tree <- tree(High ~ . - Sales, data = CarseatsH)
summary(sales_high_tree)
```

Dla drzew klasyfikacyjnych $$
  \text{deviance} = -2 n \sum_{m=1}^{|T|} \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}
$$ oraz $$
  \text{residual mean deviance} = \frac{\text{deviance}}{n - |T|}.
$$

Przedstawienie graficzne dopasowanego modelu

```{r plottree}
plot(sales_high_tree)
text(sales_high_tree, pretty = 0)
```

Więcej informacji podaje funkcja `print.tree()`

```{r print_tree}
sales_high_tree
```

[**1. Które predyktory są najbardziej istotne?**]

Metodą zbioru walidacyjnego estymujemy błąd testowy dla drzewa
klasyfikacyjnego w rozważanym problemie.

```{r classtreeerror}
set.seed(1)
n <- nrow(CarseatsH)
train <- sample(n, n / 2)
test <- -train
sales_high_tree <- tree(High ~ . - Sales, data = CarseatsH, subset = train)
tree_class <- predict(sales_high_tree, newdata = CarseatsH[test,], type = "class")
table(tree_class, CarseatsH$High[test])
mean(tree_class != CarseatsH$High[test])
```

*Duże* drzewo $T_0$ dla zbioru uczącego `CarseatsH[train,]`

```{r bigclasstree}
plot(sales_high_tree)
text(sales_high_tree, pretty = 0)
```

Załadujemy teraz ten sam zbiór danych za pomocą statsmodels

```{python carseats}
carseats = sm.datasets.get_rdataset("Carseats", "ISLR").data

carseats_h = carseats.copy()

carseats_h['High'] = ['No' if sales <= 8 else 'Yes' for sales in carseats_h['Sales']]

```

Drzewa decyzyjne i lasy losowe są w pythonie dostępne w bilbiotece
scikit-learn. Najpierw przeprowadzimy podział na zbiór treningowy i
walidacyjny

```{python classTree}
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split

X = carseats_h.drop(['Sales', 'High'], axis=1)
X = pd.get_dummies(X, drop_first=True)
y = (carseats_h['High'] == 'Yes').astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train, y_train)

y_pred = tree_model.predict(X_test)
```

Metryki do oceny modelu znajdują się w module sklearn.metrics:

```{python modelSummary}
from sklearn.metrics import classification_report, accuracy_score

print("Decision Tree Summary:")
print(f"Number of terminal nodes: {tree_model.get_n_leaves()}")
print(f"Tree depth: {tree_model.get_depth()}")
print("\nFeature importance:")
for feature, importance in zip(X.columns, tree_model.feature_importances_):
    print(f"{feature}: {importance:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

Do znalezienia optymalnego poddrzewa stosujemy przycinanie. Przy pomocy
CV konstruujemy ciąg poddrzew wyznaczony przez malejącą złożoność.

```{r classtreecv}
set.seed(1)
sales_high_cv <- cv.tree(sales_high_tree, FUN = prune.misclass)
sales_high_cv
plot(sales_high_cv$size, sales_high_cv$dev, type = "b")
```

Składowa `sales_high_cv$dev` zawiera liczbę błędów CV. Przycinamy drzewo
$T_0$ do poddrzewa z najmniejszym poziomem błędów CV.

```{r class.tree.prune}
size_opt <- sales_high_cv$size[which.min(sales_high_cv$dev)]
sales_high_pruned <- prune.misclass(sales_high_tree, best = size_opt)
plot(sales_high_pruned)
text(sales_high_pruned, pretty = 0)
```

Testowy poziom błędów dla optymalnego poddrzewa.

```{r class.pruned.error}
pruned_class <- predict(sales_high_pruned, newdata = CarseatsH[test,], 
                        type = "class")
table(pruned_class, CarseatsH$High[test])
mean(pruned_class != CarseatsH$High[test])
```

**2. Narysuj wykres błędu testowego w zależności od rozmiaru
poddrzewa.**

Tutaj przycinanie drzewa w pythonie zgodnie z dokumentacją scikit-learn.
Wykorzystana tam metoda jest trochę inna. Najmniej efektywne liście są
wycinane i tworzony jest raport, jak zmienia się ogólna entropia liści
wraz ze wzrostem alfa (parametr siły przycinania penalizujący ilość
liści). Ostatnia wartość alfa w ścieżce zostawia drzewo tylko z
korzeniem.

```{python class.tree.prune}
clf = DecisionTreeClassifier(random_state=41)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()
```

Po wybraniu odpowiedniej wartości alfa, możemy otrzymać drzewo o
zadanych właściwościach:

```{python selectAlpha}
pruned_tree = DecisionTreeClassifier(random_state=0, ccp_alpha=0.009)
pruned_tree.fit(X_train, y_train)

y_pred_pruned = tree_model.predict(X_test)

print("Decision Tree Summary:")
print(f"Number of terminal nodes: {pruned_tree.get_n_leaves()}")
print(f"Tree depth: {pruned_tree.get_depth()}")
print("\nFeature importance:")
for feature, importance in zip(X.columns, pruned_tree.feature_importances_):
    print(f"{feature}: {importance:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_pruned))
```

### Drzewa regresyjne

Używamy zbioru danych `Boston` z pakietu `MASS`. Konstruujemy drzewo
decyzyjne dla problemu regresji `medv` względem pozostałych zmiennych.

```{r regressiontree}
medv_tree <- tree(medv ~ ., data = Boston)
summary(medv_tree)
```

*Deviance* oznacza tutaj RSS. Przedstawienie drzewa

```{r medvtreeshow}
medv_tree
plot(medv_tree)
text(medv_tree)
```

**3. Które predyktory są najistotniejsze?**

Metodą zbioru walidacyjnego szacujemy błąd testowy.

```{r medvtreeerror}
set.seed(1)
n <- nrow(Boston)
train <- sample(n, n / 2)
test <- -train
medv_tree <- tree(medv ~ ., data = Boston, subset = train)
medv_pred <- predict(medv_tree, newdata = Boston[test,])
mean((medv_pred - Boston$medv[test])^2)
```

Wyznaczamy optymalne poddrzewo metodą przycinania sterowanego
złożonością.

```{r medv.tree.cv}
medv_cv <- cv.tree(medv_tree)
plot(medv_cv$size, medv_cv$dev, type = "b")
```

**3. Które poddrzewo jest optymalne? Jaki jest jego (estymowany) błąd
testowy?**

Przycinanie drzewa $T_0$ do żądanego poziomu realizuje w tym przypadku
funkcja `prune.tree()`.

```{r medv.prune}
medv_pruned <- prune.tree(medv_tree, best = 4)
plot(medv_pruned)
text(medv_pruned)
```

**4. Oblicz estymatę błędu testowego dla poddrzewa z 4 liśćmi.**

pythonowy odpowiednik drzew regresji.

```{python regressiontree}
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.metrics import mean_squared_error, r2_score


boston = sm.datasets.get_rdataset("Boston", "MASS").data

X_b = boston.drop('medv', axis=1)
y_b = boston['medv']
Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_b, y_b, test_size=0.5, random_state=1)

medv_tree = DecisionTreeRegressor(random_state=43)
medv_tree.fit(Xb_train, yb_train)

medv_pred = medv_tree.predict(Xb_test)
mse = mean_squared_error(yb_test, medv_pred)
r2 = r2_score(yb_test, medv_pred)

print(f"Number of terminal nodes: {medv_tree.get_n_leaves()}")
print(f"Tree depth: {medv_tree.get_depth()}")
print(f"Training RSS (deviance): {mean_squared_error(y_b, medv_tree.predict(X_b)) * len(y_b):.2f}")
print(f"\nTest Mean Squared Error: {mse:.2f}")
print(f"Test R-squared: {r2:.4f}")
```

## Bagging i lasy losowe

Bagging i lasy losowe implementowane są przez pakiet `randomForest`.
Oczywiście bagging jest szczególnym przypadkiem lasu losowego.

### Bagging

Bagging dla regresji `medv` względem wszystkich pozostałych w zbiorze
`Boston`.

```{r medvbag}
medv_bag <- randomForest(medv ~ ., data = Boston, mtry = 13, importance = TRUE)
medv_bag
```

Wykres błędu OOB względem liczby drzew

```{r medvbagoob}
plot(medv_bag, type = "l")
```

W przypadku regresji błąd MSE OOB dostępny jest w składowej `mse`
obiektu klasy `randomForest`. W przypadku klasyfikacji wyniki analizy
danych OOB dostępne są w składowych `err.rate` (proporcja błędów) i
`confusion` (tabela pomyłek).

Wyznaczenie ważności predyktorów

```{r medvimportance}
importance(medv_bag)
```

I stosowny obrazek

```{r medvimpplot}
varImpPlot(medv_bag)
```

Oszacowanie błędu testowego dla poprzednio wyznaczonego zbioru
walidacyjnego.

```{r medvbagvalid}
set.seed(2)
medv_bag <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13,
                         importance = TRUE)
medv_pred_bag <- predict(medv_bag, newdata = Boston[test,])
mean((medv_pred_bag - Boston$medv[test])^2)
```

**5. Czy dla zmniejszonego zbioru uczącego zmieniła się ważność
predyktorów?**

Powyższe dla mniejszej liczby hodowanych drzew

```{r medvbagvalidsmall}
set.seed(2)
medv_bag_s <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13,
                         importance = TRUE, ntree = 25)
medv_pred_bag_s <- predict(medv_bag_s, newdata = Boston[test,])
mean((medv_pred_bag_s - Boston$medv[test])^2)
```

### Lasy losowe

Domyślna wartość parametru `mtry` to $\sqrt{p}$ dla regresji i $p/3$ dla
klasyfikacji.

Oszacowanie błędu testowego dla poprzednio wyznaczonego zbioru
walidacyjnego.

```{r medvrfvalid}
set.seed(2)
medv_rf <- randomForest(medv ~ ., data = Boston, subset = train,
                         importance = TRUE)
medv_pred_rf <- predict(medv_rf, newdata = Boston[test,])
mean((medv_pred_rf - Boston$medv[test])^2)
```

**6. Co w tym przypadku można powiedzieć o istotności predyktorów?**

**7. Porównaj na wykresie błędy OOB dla baggingu i domyślnie
skonfigurowanego lasu losowego.**

Powyższe dla ręcznie ustawionego parametru $m$ (czyli `mtry`).

```{r medv.rf.valid.mtry}
set.seed(2)
medv_rf <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6,
                         importance = TRUE)
medv_pred_rf <- predict(medv_rf, newdata = Boston[test,])
mean((medv_pred_rf - Boston$medv[test])^2)
```

To samo tyczy się lasów losowych w pythonie, gdzie możemy po prostu
podmienić model pojedynczego drzewa regresji na las drzew.

```{python regressionForest}
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(
    n_estimators=500,
    max_features='sqrt',
    random_state=1
)

rf_model.fit(Xb_train, yb_train)
rf_pred = rf_model.predict(Xb_test)

mse = mean_squared_error(yb_test, rf_pred)
rmse = np.sqrt(mse)
r2 = r2_score(yb_test, rf_pred)

print("Random Forest Performance:")
print(f"Test Mean Squared Error (MSE): {mse:.2f}")
print(f"Test Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Test R-squared: {r2:.4f}")

feature_importance = pd.DataFrame({
    'Feature': X_b.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance:")
print(feature_importance)
```
