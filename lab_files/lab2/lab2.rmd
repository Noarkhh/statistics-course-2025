---
title: "lab2"
author: "Hubert Guzowski"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

### Naiwny klasyfikator bayesowski

Klasyczny klasyfikator oparty o poznane już własności prawdopodobieństwa
warunkowego, twierdzenie Bayesa i niezależność zmiennych. Przystępny
opis wprowadzający do zagadnienia można znaleźć na przykład w książce:
<https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf>\
Chapter 9: Applications to Computing 9.3: The Naive Bayes Classifier

Pracujemy na wycinku korpusu maili z firmy Enron zebranych w trakcie
śledztwa po upadku firmy.
(<https://en.wikipedia.org/wiki/Enron_Corpus#cite_ref-1>) Dane zostały
już podzielone na dwie tabele (treningową i testową) zawierające treść
maila w kolumnie 'email' oraz etykietę spam\|ham w kolumnie 'label'.

Dane możemy wygodnie wczytać w języku R:

```{r read data}
train_data <- read.csv("train_emails.csv")
test_data <- read.csv("test_emails.csv")
```

```{r summary}
summary(train_data)
```

Oraz w pythonie wykorzystując bibliotekę pandas:

```{python load data}
import pandas as pd

train_df = pd.read_csv("train_emails.csv")
test_df = pd.read_csv("test_emails.csv")
```

```{python summary}
train_df.describe()
```

W ramach laboratorium należy wypełnić brakujący kod w pliku naive_bayes
z rozszerzeniem .r albo .py i następnie wytrenować klasyfikator na
zbiorze treningowym maili oraz przewidzieć odpowiedzi na zbiorze
testowym. Oczekiwana dokładność powinna wynieść około 0.98

Kod skyptu Rowego możemy zaimportować w sposób następujący:

```{r import}
library(R6) # Biblioteka R6 pozwala korzystać z klas
source("naive_bayes.R")
```

A pythonowego odpowiednio:

```{python import}
from naive_bayes import NaiveBayes

```

```{python}

naive_bayes = NaiveBayes()
naive_bayes.fit(train_df)
naive_bayes.accuracy(test_df)
```

Po ukończeniu pierwszego zadania należy podjąć się usprawnienia
klasyfikatora. W tym celu można na przykład:\
a) usprawnić tokenizację danych tekstowych, może niektóre z nich można
pominąć\
b) zastosować wygładzenie Laplace'a, by odpowiednio potraktować
sytuacje,\
gdy w zbiorze treningowym pojawia się niewidziane wcześniej słowo c)
rozszerzyć klasyfikator tak, by brał pod uwagę ilość wystąpień wyrazów
(Multinomial Naїve Bayes)\

Jako, że obecny klasyfikator już radzi sobie bardzo dobrze, można
utrudnić zadanie poprzez podzielenie zbioru treningowego i testowego w
proporcji np. 50:50
