---
title: "Modele nieliniowe"
output: html_document
editor_options: 
  markdown: 
    wrap: 90
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(splines)
library(gam)
```

```{python setup}
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.datasets import get_rdataset
from statsmodels.gam.api import GLMGam, BSplines
from statsmodels.genmod.families import Gaussian
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
import pandas as pd
```

## Modele nieliniowe

Używamy zbioru danych `Wage` z pakietu `ISLR`.

### Regresja wielomianowa

Regresja wielomianowa stopnia 4 `wage` względem `age`.

```{r poly4}
fit_poly <- lm(wage ~ poly(age, 4), data = Wage)
summary(fit_poly)
```

Generalnie bardzo dobrym źródłem wszystkich poruszonych dziś funkcjonalności 
w pythonie jest połączenie bibliotek scikit-learn i statsmodels.

Jednakże dla regresji wielomianowej nie mamy w pythonie prostego odpowiednika
funkcji poly() dostarczającej nam od razu dopasowanie do wielomianów ortogonalnych.
Trzeba je skonstruować na przykład w oparciu o NumPy:

```{python poly4}
from numpy.polynomial.legendre import legvander
from numpy.polynomial.chebyshev import chebvander

wage_df = get_rdataset("Wage", package="ISLR").data

age = wage_df['age'].values
age_scaled = 2 * (age - age.min()) / (age.max() - age.min()) - 1

X_ortho = chebvander(age_scaled, 4)[:, 1:]  # Spróbuj dopasować też za pomocą legvander
X_ortho_with_const = sm.add_constant(X_ortho)

ortho_model = sm.OLS(wage_df['wage'], X_ortho_with_const).fit()
print(ortho_model.summary())
```

Jak widać powyżej współczynniki wielomianu są różne pomiędzy implementacją R a python
a nawet pomiędzy dwoma wybranymi wielomianami z NumPy. Jednocześnie dokładność
dopasowania (R-squared) jest taka same dla wszystkich trzech modeli. Wynika to
z różnego wielomianu ortogonalnego wykorzystanego przez te podejścia. Należy mieć
to na uwadze przy interpretacji modeli.


Regresja wielomianowa z użyciem standardowej bazy wielomianów $X, X^2, X^3, X^4$.

```{r poly4raw}
fit_poly_raw <- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)
summary(fit_poly_raw)
```

W tym wypadku możemy po prostu zastosować w pythonie transformację scikit-learn
i otrzymamy taki sam wynik.

```{python poly4}
wage_df = get_rdataset("Wage", package="ISLR").data

X = wage_df[['age']].values
y = wage_df['wage'].values

poly = PolynomialFeatures(degree=4)
X_poly = poly.fit_transform(X)

model = sm.OLS(y, X_poly).fit()
print(model.summary())
```

Powyższe modele niekoniecznie potwierdzają sens zastosowania wielomianu stopnia aż 4. W
związku z tym dopasowujemy wielomian stopnia 3.

```{r}
fit_poly_3 <- lm(wage ~ poly(age, 3), data = Wage)
summary(fit_poly_3)
```

Jak widać, model ten daje nieznacznie gorsze RSE i poprawione $R^2$. Możemy wykonać test
porównujący bezpośrednio modele wielomianowe o różnych stopniach.

```{r}
anova(fit_poly, fit_poly_3)
```

Otrzymana $p$-wartość nie pozwala uznać modelu 4 stopnia za istotnie lepszy. W takiej
sytuacji na ogół wybierzemy model stopnia 3 jako prostszy.

Obrazek dopasowania zawierający krzywe błędu standardowego.

```{r poly4plot}
age_lims <- range(Wage$age)
age_grid <- seq(age_lims[1], age_lims[2])

pred_poly <- predict(fit_poly_3, list(age = age_grid), se.fit = TRUE)
se_bands <- cbind(pred_poly$fit + 2 * pred_poly$se.fit, pred_poly$fit - 2 * pred_poly$se.fit)

plot(wage ~ age, data = Wage, col = "darkgrey", cex = 0.5, xlim = age_lims)
lines(age_grid, pred_poly$fit, col = "red", lwd = 2)
matlines(age_grid, se_bands, col = "red", lty = "dashed")
```

### Regresja logistyczna wielomianowa

Chcemy skonstruować klasyfikator z dwoma klasami:

-   dużo zarabiających (więcej niż 250 000 dolarów, tzn. `wage > 250`)

-   mało zarabiających (pozostali).

Predyktorem jest `age`, ale chcemy też uwzględnić wpływ wyższych potęg (do 4) tej
zmiennej.

```{r logisticPoly}
fit_log_poly <- glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
summary(fit_log_poly)
```

W pythonie wykorzystamy implementację uogólnoinych modeli liniowych z statsmodels.

```{python logisticPoly}
wage_df['high_wage'] = (wage_df['wage'] > 250).astype(int)

age = wage_df['age'].values
age_scaled = 2 * (age - age.min()) / (age.max() - age.min()) - 1
X_ortho = chebvander(age_scaled, 4)[:, 1:]

X_ortho_with_const = sm.add_constant(X_ortho)

logit_model = sm.GLM(wage_df['high_wage'], 
                     X_ortho_with_const, 
                     family=sm.families.Binomial()).fit()

print(logit_model.summary())
```

Funkcja `predict.glm()` standardowo zwraca szanse logarytmiczne, co jest korzystne z
punktu widzenia zobrazowania błędu standardowego. Musimy jednak otrzymane wartości
przekształcić funkcją logistyczną.

```{r logisticPolyPred}
pred_log_poly <- predict(fit_log_poly, list(age = age_grid), se.fit = TRUE)
pred_probs <- plogis(pred_log_poly$fit)

se_bands_logit <- cbind(pred_log_poly$fit + 2 * pred_log_poly$se.fit, 
                        pred_log_poly$fit - 2 * pred_log_poly$se.fit)
se_bands <- plogis(se_bands_logit)

plot(Wage$age, I(Wage$wage > 250), xlim = age_lims, ylim = c(0, 1), 
     col = "darkgrey", cex = 0.5, xlab = "age", ylab = "P(wage > 250 | age)")
lines(age_grid, pred_probs, col = "red", lwd = 2)
matlines(age_grid, se_bands, lty = "dashed", col = "red")
```

"Powiększenie" rysunku można uzyskać np. ograniczając zakres `y` (parametr `ylim`).

```{r}
plot(Wage$age, I(Wage$wage > 250), xlim = age_lims, ylim = c(0, 0.2), 
     col = "darkgrey", cex = 0.5, xlab = "age", ylab = "P(wage > 250 | age)")
lines(age_grid, pred_probs, col = "red", lwd = 2)
matlines(age_grid, se_bands, lty = "dashed", col = "red")
```

Analogicznie z predykcją w pythonie, ale wypisując już konkretne dane:

```{python logisticPolyPred}
age_grid = np.linspace(wage_df['age'].min(), wage_df['age'].max(), 10)

age_min, age_max = wage_df['age'].min(), wage_df['age'].max()
age_grid_scaled = 2 * (age_grid - age_min) / (age_max - age_min) - 1

X_grid_ortho = chebvander(age_grid_scaled, 4)[:, 1:]
X_grid_ortho_with_const = sm.add_constant(X_grid_ortho)

predictions = logit_model.get_prediction(X_grid_ortho_with_const)
pred_summary = predictions.summary_frame(alpha=0.05)  # 95% przedziały ufności

results = pd.DataFrame({
    'age': age_grid,
    'pred_logit': pred_summary['mean'],
    'se_logit': pred_summary['mean_se'],
    'pred_prob': pred_summary['mean'],
    'ci_lower': pred_summary['mean_ci_lower'],
    'ci_upper': pred_summary['mean_ci_upper']
})

print("Predykcje i przedziały ufności:")
print(results.round(4))
```

Z tabeli pandas możemy wykonać następnie wizualizację za pomocą na przykład matplotlib,
seaborn czy plotly.

### Funkcje schodkowe

Dopasowanie funkcji schodkowej wykonujemy przy pomocy funkcji `cut()` przekształcającej
zmienną numeryczną w czynnik.

```{r cut}
cut(Wage$age, breaks = 4) |> table()
```

Samo dopasowanie wykonuje funkcja `lm()`.

```{r step}
fit_step <- lm(wage ~ cut(age, 4), data = Wage)
summary(fit_step)
```

Stosowny obrazek.

```{r}
pred_step <- predict(fit_step, list(age = age_grid), se.fit = TRUE)
se_bands <- cbind(pred_step$fit + 2 * pred_step$se.fit, pred_step$fit - 2 * pred_step$se.fit)

plot(wage ~ age, data = Wage, col = "darkgrey", cex = 0.5, xlim = age_lims)
lines(age_grid, pred_step$fit, col = "red", lwd = 2)
matlines(age_grid, se_bands, col = "red", lty = "dashed")
```

W pythonie przekształcenie w czynnik wykonujemy np. za pomocą pandas:

```{python cut}
wage_df['age_cut'] = pd.cut(wage_df['age'], bins=4)
```

I następnie dopasowujemy po prostu regresję:

```{python step}
model_step = smf.ols('wage ~ C(age_cut)', data=wage_df).fit()

print(model_step.summary())
```

### Funkcje sklejane

Bazę regresyjnych funkcji sklejanych wylicza funkcja `bs()` z pakietu `splines`. Domyślnym
stopniem funkcji sklejanych jest 3.

Regresja z użyciem funkcji sklejanych z ustalonymi węzłami.

```{r bsFixedKnots}
fixed_knots <- c(25, 40, 60)
fit_bs_knots <- lm(wage ~ bs(age, knots = fixed_knots), data = Wage)
summary(fit_bs_knots)
```

Obrazek.

```{r}
pred_bs_knots <- predict(fit_bs_knots, list(age = age_grid), se.fit = TRUE)
se_bands <- cbind(pred_bs_knots$fit + 2 * pred_bs_knots$se.fit, 
                  pred_bs_knots$fit - 2 * pred_bs_knots$se.fit)

plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
lines(age_grid, pred_bs_knots$fit, col = "red", lwd = 2)
matlines(age_grid, se_bands, col = "red", lty = "dashed")
abline(v = fixed_knots, lty = "dotted")
```

```{python bsFixedKnots}
#patsy to inspirowany R pakiet do opisu modeli statystycznych i tworzenia formuł
#jest częścią statsmodels
from patsy import dmatrix

fixed_knots = [25, 40, 60]

# Domyślne spline'y w R mają 3 stopień
bs_basis = dmatrix("bs(age, knots=fixed_knots, degree=3, include_intercept=False)",
                   {"age": wage_df['age'], "fixed_knots": fixed_knots},
                   return_type='dataframe')

X = sm.add_constant(bs_basis)
fit_bs_knots = sm.OLS(wage_df['wage'], X).fit()
fit_bs_knots.summary()
```

#### Problem

**Sprawdź jak ustawienie węzłów wpływa na dopasowany model.**

Dopasowanie modelu wykorzystującego funkcje sklejane o ustalonej liczbie stopni swobody.
Węzły są rozmieszczane automatycznie.

```{r bsFixedDF}
fit_bs_df <- lm(wage ~ bs(age, df = 6), data = Wage)
summary(fit_bs_df)
```

Obrazek.

```{r}
pred_bs_df <- predict(fit_bs_df, list(age = age_grid), se.fit = TRUE)
se_bands <- cbind(pred_bs_df$fit + 2 * pred_bs_df$se.fit, pred_bs_df$fit - 2 * pred_bs_df$se.fit)

plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
lines(age_grid, pred_bs_df$fit, col = "red", lwd = 2)
matlines(age_grid, se_bands, col = "red", lty = "dashed")

bs_knots <- attr(bs(Wage$age, df = 6), "knots")
abline(v = bs_knots, lty = "dotted")
```

Oraz wersja pythonowa
```{python bsFixedDF}
bs_basis = dmatrix("bs(age, df=6, degree=3, include_intercept=False)",
                  {"age": wage_df['age']},
                  return_type='dataframe')


X = sm.add_constant(bs_basis)
fit_bs_df = sm.OLS(wage_df['wage'], X).fit()

fit_bs_df.summary()
```

#### Problemy

-   **Sprawdź jak liczba stopni swobody wpływa na dopasowany model.**

-   **Funkcja `bs()` akceptuje parametr `degree`, który ustala stopień funkcji sklejanej.
    Sprawdź jak w powyższych przykładach wyglądają funkcje sklejane innych stopni.
    W przypadku pythona możesz sformatować tekst formuły.**

### Naturalne funkcje sklejane

Bazę naturalnych **sześciennych** (czyli stopnia 3) funkcji sklejanych wyznacza funkcja
`ns()` z pakietu `splines`.

```{r ns}
fit_ns <- lm(wage ~ ns(age, df = 4), data = Wage)
summary(fit_ns)
```

Obrazek.

```{r}
pred_ns <- predict(fit_ns, list(age = age_grid), se.fit = TRUE)
se_bands <- cbind(pred_ns$fit + 2 * pred_ns$se.fit, pred_ns$fit - 2 * pred_ns$se.fit)

plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
lines(age_grid, pred_ns$fit, col = "red", lwd = 2)
matlines(age_grid, se_bands, col = "red", lty = "dashed")
abline(v = attr(ns(Wage$age, df = 4), "knots"), lty = "dotted")
```

Natomiast w pythonie formułę patsy możemy zmienić formatując string:

```{python bsVariedDF}
df = 6

bs_basis = dmatrix(f"bs(age, df={df}, degree=3, include_intercept=False)",
                  {"age": wage_df['age']},
                  return_type='dataframe')


X = sm.add_constant(bs_basis)
fit_bs_df = sm.OLS(wage_df['wage'], X).fit()

fit_bs_df.summary()
```

### Wygładzające funkcje sklejane

Dopasowanie wygładzającej (sześciennej) funkcji sklejanej do danych wykonuje funkcja
`smooth.spline()`. Możemy dopasować wygładzającą funkcję sklejaną o ustalonej liczbie
stopni swobody (tu 16).

```{r smooth}
fit_smooth_df <- smooth.spline(Wage$age, Wage$wage, df = 16)
plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
lines(fit_smooth_df, col = "red", lwd = 2)
```

Można też liczbę stopni swobody wyznaczyć automatycznie korzystając z walidacji krzyżowej.

```{r smoothcv, warning=FALSE}
fit_smooth_cv <- smooth.spline(Wage$age, Wage$wage, cv = TRUE)
plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
lines(fit_smooth_cv, col = "red", lwd = 2)
```

```{python smooth}
import matplotlib.pyplot as plt # Tym razem wizualizacja z wykorzystaniem matplotlib

x = wage_df["age"]
y = wage_df["wage"]


x_grid = np.linspace(x.min(), x.max(), 200)
bs_basis = dmatrix("bs(age, df=10, degree=3, include_intercept=False)", {"age": x}, return_type='dataframe')
gam_bs = GLMGam(y, smoother=BSplines(x.values[:, None], df=[10], degree=[3]), exog=np.ones((len(x), 1)), family=Gaussian()).fit()

preds = gam_bs.predict(exog=np.ones((len(x_grid), 1)), exog_smooth=x_grid[:, None])


plt.scatter(x, y, color='gray', alpha=0.5, s=10)
plt.plot(x_grid, preds, color='red', lw=2)
plt.xlabel("Age")
plt.ylabel("Wage")
plt.ylim(0, 200)
plt.show()
```

### Regresja lokalna

Regresję lokalną (domyślnie wielomianami stopnia 2) wykonuje funkcja `loess()`. Parametr
funkcji o nazwie `span` odpowiada parametrowi metody $s$.

```{r localReg}
spans <- c(0.2, 0.5)
clrs <- c("red", "blue")
plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
for (i in 1:length(spans)) {
   fit_loess <- loess(wage ~ age, span = spans[i], data = Wage)
   pred_loess <- predict(fit_loess, data.frame(age = age_grid))
   lines(age_grid, pred_loess, col = clrs[i], lwd = 2)
}
legend("topright", legend = paste("s =", spans), col = clrs, lty = 1, lwd = 2)
```

Regresja lokalna wykonana przy pomocy wielomianów stopnia 1 (funkcji afinicznych).

```{r localReg1}
spans <- c(0.2, 0.5)
clrs <- c("red", "blue")
plot(wage ~ age, data = Wage, cex = 0.5, col = "darkgrey")
for (i in 1:length(spans)) {
   fit_loess <- loess(wage ~ age, span = spans[i], degree = 1, data = Wage)
   pred_loess <- predict(fit_loess, data.frame(age = age_grid))
   lines(age_grid, pred_loess, col = clrs[i], lwd = 2)
}
legend("topright", legend = paste("s =", spans), col = clrs, lty = 1, lwd = 2)
```

W statsmodels podobnie mamy do dyspozycji funkcję lowess implementującą
regresję lokalną:

```{python localReg}
from statsmodels.nonparametric.smoothers_lowess import lowess

age = wage_df['age']
wage = wage_df['wage']
age_grid = np.linspace(age.min(), age.max(), 100)

spans = [0.2, 0.5]
colors = ['red', 'blue']

plt.scatter(age, wage, color='darkgrey', s=5)

for span, color in zip(spans, colors):
    loess_result = lowess(wage, age, frac=span, it=0, return_sorted=True)
    plt.plot(loess_result[:, 0], loess_result[:, 1], color=color, lw=2)

plt.legend([f's = {s}' for s in spans], loc='upper right')
plt.xlabel('age')
plt.ylabel('wage')
plt.show()
```

## Uogólnione modele addytywne (GAM)

GAM wykorzystujący wyłącznie modele parametryczne (czyli metodę funkcji bazowych) może być
uczony metodą najmniejszych kwadratów przy pomocy funkcji `lm()`.

```{r gamls}
fit_gam_ls <- lm(wage ~ ns(year, df = 4) + ns(age, df = 5) + education, data = Wage)
summary(fit_gam_ls)
```

No i GAMy w wersji pythonowej statsmodels + patsy:

```{python gamls}
X = dmatrix("bs(year, df=4, include_intercept=False) + bs(age, df=5, include_intercept=False) + C(education)",
            data=wage_df, return_type='dataframe')

y = wage_df['wage']

model = sm.OLS(y, X).fit()
model.summary()
```

GAM ze składnikami nieparametrycznymi są uczone np. przy pomocy algorytmu dopasowania
wstecznego zaimplementowanego w funkcji `gam()` z pakietu `gam`. Pakiet `gam` zawiera
funkcje pomocnicze realizujące składniki nieparametryczne:

-   `s()` dla wygładzających funkcji sklejanych i

-   `lo()` dla lokalnej regresji.

Dopasowanie modelu podobnego do poprzedniego, ale z użyciem wygładzających funkcji
sklejanych.

```{r gambf}
fit_gam <- gam(wage ~ s(year) + s(age) + education, data = Wage)
summary(fit_gam)
```

Statsmodels wspiera podobną funkcjonalność (poniżej), ale nie mamy dokładnego odpowiednika.
Dla wierniejszego odwzorowania należałoby w pythonie doinstalować pakiet pygam.

```{python gambf}
y = wage_df['wage']
X_lin = dmatrix("C(education)", data=wage_df, return_type='dataframe')
x_smooth = wage_df[['year', 'age']]

bs = BSplines(x_smooth, df=[5, 5], degree=[3, 3])

gam_model = GLMGam(y, exog=X_lin, smoother=bs).fit()
gam_model.summary()
```

Raport tym razem nie zawiera wartości RSE i $R^2$, tylko odchylenie i AIC. Wynik testu
ANOVA dla efektów parametrycznych daje mocne podstawy do twierdzenia o istotności
wszyskich predyktorów. Natomiast test ANOVA dla efektów nieparametrycznych potwierdza
istotność składnika nieliniowego względem `age`, ale nie potwierdza istotności składnika
nieliniowego względem `year`. W konsekwencji, mamy podstawy oprzeć analizę na modelu
prostszym.

```{r}
fit_gam_simpler <- gam(wage ~ year + s(age) + education, data = Wage)
summary(fit_gam_simpler)
```

Funkcja `plot.Gam()` wykonuje wykres zwany **wykresem częściowej zależności** [*Partial
Dependency Plot, PDP*] dla każdego predyktora osobno. Jest to rodzaj wykresu bardzo
użyteczny w interpretacji modeli nieliniowych, a szczególnie łatwo wykonywalny w przypadku
modeli addytywnych.

PDP dla modelu `fit_gam` wygląda następujaco.

```{r gambfplot}
plot(fit_gam, col = "red", se = TRUE)
```

PDP dla modelu prostszego.

```{r}
plot(fit_gam_simpler, col = "red", se = TRUE)
```

Funkcja `plot.Gam()` działa też dla modeli metody najmniejszych kwadratów, ale wówczas
trzeba się do niej odwołać jawnie.

```{r gamlsplot}
plot.Gam(fit_gam_ls, col = "red", se = TRUE)
```

Istnieje wersja funkcji `anova()` porównująca modele GAM.

```{r anovagam}
fit_gam_ae <- gam(wage ~ s(age) + education, data = Wage)
anova(fit_gam_ae, fit_gam_simpler, fit_gam)
```

Test wykazał wyższość modeli 2 i 3 nad modelem 1 (najprostszym), natomiast nie wykazał
istotnej różnicy w działaniu między modelami 2 i 3.

Dopasowanie modelu wykorzystującego lokalną regresję.

```{r gamlo}
fit_gam_lo <- gam(wage ~ s(year) + lo(age) + education, data = Wage)
summary(fit_gam_lo)
```

Wykres częściowej zależności.

```{r}
plot(fit_gam_lo, col = "green", se = TRUE)
```

Porównanie z innymi modelami.

```{r}
anova(fit_gam_ae, fit_gam_simpler, fit_gam_lo)
```

### GAM w GLM

Regresja logistyczna wykorzystująca GAM

```{r logisticgam}
fit_logistic_gam <- gam(I(wage > 250) ~ year + s(age) + education, 
                        family = binomial, data = Wage)
summary(fit_logistic_gam)
```

Wykres częściowej zależności.

```{r}
plot(fit_logistic_gam, col = "blue", se = TRUE)
```
